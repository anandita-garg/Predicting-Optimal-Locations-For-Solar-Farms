{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjqCXdhMp/WULVUSWljyxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandita-garg/Predicting-Optimal-Locations-For-Solar-Farms/blob/main/API_Calls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "#Getting Coordinates\n",
        "\n",
        "# Load world shapefile and extract India\n",
        "world = gpd.read_file('ne_10m_admin_0_countries.shp')\n",
        "india = world[world['ADMIN'] == 'India']\n",
        "\n",
        "lat_min, lat_max = 6.462, 37.087\n",
        "lon_min, lon_max = 68.175, 97.417\n",
        "\n",
        "def generate_random_coordinate():\n",
        "    lat = random.uniform(lat_min, lat_max)\n",
        "    lon = random.uniform(lon_min, lon_max)\n",
        "    return lat, lon\n",
        "\n",
        "def is_valid_coordinate(new_coord, existing_coords, min_distance_m=30):\n",
        "    point = Point(new_coord[1], new_coord[0])\n",
        "    if not india.contains(point).any():\n",
        "        return False\n",
        "    for coord in existing_coords:\n",
        "        if geodesic(new_coord, coord).meters < min_distance_m:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def generate_coordinates(num_coords=6000):\n",
        "    coordinates = []\n",
        "    attempts = 0\n",
        "    for _ in tqdm(range(num_coords), desc=\"Generating Coordinates\", ncols=100):\n",
        "        while True:\n",
        "            new_coord = generate_random_coordinate()\n",
        "            if is_valid_coordinate(new_coord, coordinates):\n",
        "                coordinates.append(new_coord)\n",
        "                break\n",
        "            attempts += 1\n",
        "            if attempts > num_coords * 10:\n",
        "                print(\"Too many attempts â€” stopping early.\")\n",
        "                break\n",
        "    return coordinates\n",
        "\n",
        "# Generate and save\n",
        "coords = generate_coordinates(6000)\n",
        "df = pd.DataFrame(coords, columns=['latitude', 'longitude'])\n",
        "df.to_csv('test.csv', index=False)"
      ],
      "metadata": {
        "id": "r29BUKKbweAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRK9QqrTqYdM"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.neighbors import BallTree\n",
        "from functools import lru_cache\n",
        "\n",
        "\n",
        "#API call to retrieve features for AHP1\n",
        "\n",
        "\n",
        "CUSTOM_LULC_CLASSES = {\n",
        "    5: \"Built-up Areas\", 4: \"Water Bodies\", 3: \"Forest\",\n",
        "    2: \"Vegetation\", 1: \"Wasteland/Barren Areas\"\n",
        "}\n",
        "\n",
        "ESA_TO_CUSTOM_LULC = {\n",
        "    50: 5, 80: 4, 10: 3, 20: 3, 40: 2, 30: 2,\n",
        "    60: 1, 70: 1, 90: 1, 95: 1, 100: 1\n",
        "}\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='test')\n",
        "\n",
        "df = pd.read_csv(\"test.csv\")\n",
        "print(f\"Loaded {len(df)} coordinates from CSV\")\n",
        "\n",
        "def get_lulc(lat, lon):\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "    esa_lulc = ee.Image(\"ESA/WorldCover/v100/2020\").select(\"Map\")\n",
        "    remapped = esa_lulc.remap(list(ESA_TO_CUSTOM_LULC.keys()),\n",
        "                             list(ESA_TO_CUSTOM_LULC.values())).rename(\"Custom_LULC\")\n",
        "    lulc_value = remapped.sample(region=point, scale=10).first().get(\"Custom_LULC\").getInfo()\n",
        "    return lulc_value, CUSTOM_LULC_CLASSES.get(lulc_value, \"Unknown\")\n",
        "\n",
        "def get_elevation(lat, lon):\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "    srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
        "    elevation = srtm.sample(region=point, scale=30).first().get(\"elevation\").getInfo()\n",
        "    return elevation\n",
        "\n",
        "def get_slope(lat, lon):\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "    srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
        "    terrain = ee.Terrain.products(srtm)\n",
        "    slope = terrain.select(\"slope\")\n",
        "    slope_value = slope.sample(region=point, scale=30).first().get(\"slope\").getInfo()\n",
        "    return slope_value\n",
        "\n",
        "def get_cloud_cover(lat, lon):\n",
        "    start_date = \"20240410\"\n",
        "    end_date = \"20250410\"\n",
        "    url = (\n",
        "        f\"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
        "        f\"?parameters=CLOUD_AMT\"\n",
        "        f\"&community=RE\"\n",
        "        f\"&latitude={lat}\"\n",
        "        f\"&longitude={lon}\"\n",
        "        f\"&start={start_date}\"\n",
        "        f\"&end={end_date}\"\n",
        "        f\"&format=JSON\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        return None, None\n",
        "\n",
        "    data = response.json()\n",
        "    cloud_data = data[\"properties\"][\"parameter\"][\"CLOUD_AMT\"]\n",
        "    df_cloud = pd.DataFrame(cloud_data.items(), columns=[\"date\", \"cloud_cover\"])\n",
        "\n",
        "    avg_cloud_cover = df_cloud[\"cloud_cover\"].mean()\n",
        "    cloudy_days = (df_cloud[\"cloud_cover\"] > 70).sum()\n",
        "    cloudy_percent = (cloudy_days / len(df_cloud)) * 100\n",
        "\n",
        "    return avg_cloud_cover, cloudy_percent\n",
        "\n",
        "def get_humidity(lat, lon):\n",
        "    start_date = \"2024-04-10\"\n",
        "    end_date = \"2025-04-10\"\n",
        "    url = (\n",
        "        \"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "        f\"latitude={lat}&longitude={lon}\"\n",
        "        f\"&start_date={start_date}&end_date={end_date}\"\n",
        "        \"&daily=relative_humidity_2m_mean\"\n",
        "        \"&timezone=auto\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"daily\" not in data:\n",
        "        return None\n",
        "\n",
        "    humidity = data[\"daily\"][\"relative_humidity_2m_mean\"]\n",
        "    return sum(humidity) / len(humidity) if humidity else None\n",
        "\n",
        "def get_temperature(lat, lon):\n",
        "    start_date = \"2024-04-10\"\n",
        "    end_date = \"2025-04-10\"\n",
        "    url = (\n",
        "        \"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "        f\"latitude={lat}&longitude={lon}\"\n",
        "        f\"&start_date={start_date}&end_date={end_date}\"\n",
        "        \"&daily=temperature_2m_mean\"\n",
        "        \"&timezone=auto\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"daily\" not in data:\n",
        "        return None\n",
        "\n",
        "    temps = data[\"daily\"][\"temperature_2m_mean\"]\n",
        "    return sum(temps) / len(temps) if temps else None\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_solar_radiation(lat, lon):\n",
        "    start_date = \"2024-01-01\"\n",
        "    end_date = \"2024-12-31\"\n",
        "    url = (\n",
        "        f\"https://archive-api.open-meteo.com/v1/archive?latitude={lat}&longitude={lon}\"\n",
        "        f\"&start_date={start_date}&end_date={end_date}&daily=shortwave_radiation_sum\"\n",
        "        f\"&timezone=auto\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()['daily']['shortwave_radiation_sum']\n",
        "        return np.nanmean(data)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_distance_to_road(lat, lon, bbox_width=0.1):\n",
        "    bbox = (\n",
        "        lon - bbox_width/2, lat - bbox_width/2,\n",
        "        lon + bbox_width/2, lat + bbox_width/2\n",
        "    )\n",
        "\n",
        "    query = f\"\"\"\n",
        "    [out:json][timeout:300];\n",
        "    way[highway]({bbox[1]},{bbox[0]},{bbox[3]},{bbox[2]});\n",
        "    node(w);\n",
        "    out;\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.post(\"https://overpass-api.de/api/interpreter\", data={'data': query})\n",
        "    data = response.json()\n",
        "\n",
        "    road_coords = []\n",
        "    for element in data['elements']:\n",
        "        if element['type'] == 'node':\n",
        "            road_coords.append([element['lat'], element['lon']])\n",
        "\n",
        "    if not road_coords:\n",
        "        if bbox_width < 0.5:\n",
        "            return get_distance_to_road(lat, lon, bbox_width * 2)\n",
        "        return None\n",
        "\n",
        "    road_coords_rad = np.deg2rad(road_coords)\n",
        "    tree = BallTree(road_coords_rad, metric='haversine')\n",
        "\n",
        "    test_coord_rad = np.deg2rad([[lat, lon]])\n",
        "    distances, indices = tree.query(test_coord_rad, k=1)\n",
        "\n",
        "    distance_km = distances[0][0] * 6371\n",
        "    return distance_km\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    lat = row['latitude']\n",
        "    lon = row['longitude']\n",
        "    print(f\"\\n==== Processing coordinate {i+1}/{len(df)}: ({lat}, {lon}) ====\")\n",
        "\n",
        "    features_added = []\n",
        "    try:\n",
        "        lulc_code, lulc_type = get_lulc(lat, lon)\n",
        "        df.at[i, 'lulc_code'] = lulc_code\n",
        "        df.at[i, 'lulc_type'] = lulc_type\n",
        "        features_added.append(f\"LULC: {lulc_type} (code {lulc_code})\")\n",
        "\n",
        "        elevation = get_elevation(lat, lon)\n",
        "        df.at[i, 'elevation'] = elevation\n",
        "        features_added.append(f\"Elevation: {elevation} meters\")\n",
        "\n",
        "        slope = get_slope(lat, lon)\n",
        "        df.at[i, 'slope'] = slope\n",
        "        features_added.append(f\"Slope: {slope:.2f}Â°\")\n",
        "\n",
        "        cloud_cover, cloudy_days_percent = get_cloud_cover(lat, lon)\n",
        "        df.at[i, 'cloud_cover'] = cloud_cover\n",
        "        df.at[i, 'cloudy_days_percent'] = cloudy_days_percent\n",
        "        if cloud_cover is not None:\n",
        "            features_added.append(f\"Cloud cover: {cloud_cover:.2f}%, Cloudy days: {cloudy_days_percent:.2f}%\")\n",
        "\n",
        "        humidity = get_humidity(lat, lon)\n",
        "        df.at[i, 'humidity'] = humidity\n",
        "        if humidity is not None:\n",
        "            features_added.append(f\"Humidity: {humidity:.2f}%\")\n",
        "\n",
        "        temperature = get_temperature(lat, lon)\n",
        "        df.at[i, 'temperature'] = temperature\n",
        "        if temperature is not None:\n",
        "            features_added.append(f\"Temperature: {temperature:.2f}Â°C\")\n",
        "\n",
        "        solar = get_solar_radiation(lat, lon)\n",
        "        df.at[i, 'solar_radiation'] = solar\n",
        "        if solar is not None:\n",
        "            features_added.append(f\"Solar radiation: {solar:.2f} kWh/mÂ²\")\n",
        "\n",
        "        distance = get_distance_to_road(lat, lon)\n",
        "        df.at[i, 'distance_to_road'] = distance\n",
        "        if distance is not None:\n",
        "            features_added.append(f\"Distance to road: {distance:.3f} km\")\n",
        "\n",
        "        print(f\"For coordinate ({lat}, {lon}), the following features have been added:\")\n",
        "        for feature in features_added:\n",
        "            print(f\"  - {feature}\")\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing coordinate {i+1}: {e}\")\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        df.to_csv(\"test.csv\", index=False)\n",
        "        print(f\"Progress saved ({i+1}/{len(df)} coordinates processed)\")\n",
        "\n",
        "df.to_csv(\"test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ee\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='test')\n",
        "\n",
        "#API call to retrieve remaining features for AHP2\n",
        "def get_ndvi_class(lat, lon, start_date, end_date):\n",
        "    try:\n",
        "        collection = ee.ImageCollection(\"MODIS/061/MOD13Q1\").filterDate(start_date, end_date)\n",
        "        image = collection.sort('system:time_start', False).first()\n",
        "        ndvi_image = image.select('NDVI')\n",
        "        point = ee.Geometry.Point(lon, lat)\n",
        "        value = ndvi_image.sample(region=point, scale=250).first().get('NDVI').getInfo()\n",
        "        if value is None:\n",
        "            return None, None\n",
        "        ndvi = value / 10000\n",
        "        if ndvi < 0.1:\n",
        "            return ndvi, \"Barren/Urban\"\n",
        "        elif ndvi < 0.3:\n",
        "            return ndvi, \"Sparse\"\n",
        "        elif ndvi < 0.6:\n",
        "            return ndvi, \"Moderate\"\n",
        "        else:\n",
        "            return ndvi, \"Dense\"\n",
        "    except Exception as e:\n",
        "        print(f\"NDVI error @({lat},{lon}): {e}\")\n",
        "        return None, None\n",
        "\n",
        "def get_aspect(lat, lon):\n",
        "    try:\n",
        "        dem = ee.Image(\"USGS/SRTMGL1_003\")\n",
        "        aspect = ee.Terrain.aspect(dem)\n",
        "        point = ee.Geometry.Point([lon, lat])\n",
        "        val = aspect.sample(region=point, scale=30).first().get('aspect').getInfo()\n",
        "        return val\n",
        "    except Exception as e:\n",
        "        print(f\"Aspect error @({lat},{lon}): {e}\")\n",
        "        return None\n",
        "\n",
        "def classify_aspect(aspect):\n",
        "    if aspect == -1:\n",
        "        return 0, \"Flat\"\n",
        "    elif 0 <= aspect <= 22.5 or 337.5 < aspect <= 360:\n",
        "        return 0, \"North\"\n",
        "    elif 22.5 < aspect <= 67.5:\n",
        "        return 0.25, \"Northeast\"\n",
        "    elif 67.5 < aspect <= 112.5:\n",
        "        return 0.5, \"East\"\n",
        "    elif 112.5 < aspect <= 157.5:\n",
        "        return 0.75, \"Southeast\"\n",
        "    elif 157.5 < aspect <= 202.5:\n",
        "        return 1, \"South\"\n",
        "    elif 202.5 < aspect <= 247.5:\n",
        "        return 0.75, \"Southwest\"\n",
        "    elif 247.5 < aspect <= 292.5:\n",
        "        return 0.5, \"West\"\n",
        "    elif 292.5 < aspect <= 337.5:\n",
        "        return 0.25, \"Northwest\"\n",
        "    else:\n",
        "        return None, \"Unknown\"\n",
        "\n",
        "def get_lst(lat, lon, start_date, end_date):\n",
        "    try:\n",
        "        collection = ee.ImageCollection(\"MODIS/061/MOD11A2\").filterDate(start_date, end_date)\n",
        "        image = collection.sort('system:time_start', False).first()\n",
        "        lst_image = image.select('LST_Day_1km')\n",
        "        point = ee.Geometry.Point(lon, lat)\n",
        "        value = lst_image.sample(region=point, scale=1000).first().get('LST_Day_1km').getInfo()\n",
        "        if value is None:\n",
        "            return None\n",
        "        return value * 0.02 - 273.15\n",
        "    except Exception as e:\n",
        "        print(f\"LST error @({lat},{lon}): {e}\")\n",
        "        return None\n",
        "\n",
        "def retry_request(url, params, max_retries=3, delay=1):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "        except Exception:\n",
        "            time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "def get_wind_speed(lat, lon, start_date, end_date):\n",
        "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
        "    params = {\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"start_date\": start_date,\n",
        "        \"end_date\": end_date,\n",
        "        \"daily\": \"windspeed_10m_max,windspeed_10m_min\",\n",
        "        \"timezone\": \"auto\"\n",
        "    }\n",
        "    data = retry_request(url, params)\n",
        "    if not data or \"daily\" not in data:\n",
        "        return None\n",
        "    try:\n",
        "        max_ws = data[\"daily\"][\"windspeed_10m_max\"]\n",
        "        min_ws = data[\"daily\"][\"windspeed_10m_min\"]\n",
        "        ws = [(x + y) / 2 for x, y in zip(max_ws, min_ws)]\n",
        "        return np.mean(ws)\n",
        "    except Exception as e:\n",
        "        print(f\"Wind parsing error @({lat},{lon}): {e}\")\n",
        "        return None\n",
        "\n",
        "def process_csv(input_csv, output_csv, start_date, end_date):\n",
        "    df = pd.read_csv(input_csv)\n",
        "    print(f\"\\nLoaded {len(df)} rows from {input_csv}\")\n",
        "\n",
        "    if os.path.exists(output_csv):\n",
        "        df_out = pd.read_csv(output_csv)\n",
        "        print(f\"Found existing output file '{output_csv}' with {len(df_out)} rows.\")\n",
        "    else:\n",
        "        df_out = df.copy()\n",
        "        df_out['NDVI'] = np.nan\n",
        "        df_out['Vegetation Class'] = None\n",
        "        df_out['Aspect Value'] = np.nan\n",
        "        df_out['Aspect Class'] = np.nan\n",
        "        df_out['Aspect Direction'] = None\n",
        "        df_out['Wind Speed (m/s)'] = np.nan\n",
        "        df_out['LST (Â°C)'] = np.nan\n",
        "        print(f\"Creating new output file '{output_csv}'.\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        lat, lon = row['latitude'], row['longitude']\n",
        "        print(f\"\\nProcessing point {idx + 1}/{len(df)}: ({lat}, {lon})\")\n",
        "\n",
        "        existing = df_out.iloc[idx]\n",
        "        need_update = (\n",
        "            pd.isna(existing['NDVI']) or\n",
        "            pd.isna(existing['Aspect Value']) or\n",
        "            pd.isna(existing['Wind Speed (m/s)']) or\n",
        "            pd.isna(existing['LST (Â°C)'])\n",
        "        )\n",
        "\n",
        "        if not need_update:\n",
        "            print(\"Already processed. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        ndvi, veg_class = get_ndvi_class(lat, lon, start_date, end_date)\n",
        "        aspect_val = get_aspect(lat, lon)\n",
        "        aspect_class, aspect_dir = classify_aspect(aspect_val) if aspect_val is not None else (None, None)\n",
        "        wind_speed = get_wind_speed(lat, lon, start_date, end_date)\n",
        "        lst = get_lst(lat, lon, start_date, end_date)\n",
        "\n",
        "        df_out.at[idx, 'NDVI'] = ndvi\n",
        "        df_out.at[idx, 'Vegetation Class'] = veg_class\n",
        "        df_out.at[idx, 'Aspect Value'] = aspect_val\n",
        "        df_out.at[idx, 'Aspect Class'] = aspect_class\n",
        "        df_out.at[idx, 'Aspect Direction'] = aspect_dir\n",
        "        df_out.at[idx, 'Wind Speed (m/s)'] = wind_speed\n",
        "        df_out.at[idx, 'LST (Â°C)'] = lst\n",
        "\n",
        "        print(f\"NDVI: {ndvi}, Vegetation: {veg_class}\")\n",
        "        print(f\"Aspect: {aspect_val}Â°, Direction: {aspect_dir}\")\n",
        "        print(f\"Wind speed: {wind_speed:.2f} m/s\" if wind_speed else \"Wind speed: Failed to fetch\")\n",
        "        print(f\"LST: {lst:.2f} Â°C\" if lst else \"LST: Failed to fetch\")\n",
        "\n",
        "        df_out.to_csv(output_csv, index=False)\n",
        "\n",
        "    print(f\"\\nAll coordinates processed. Results saved to '{output_csv}'.\")\n",
        "\n",
        "process_csv(\"test\", \"test2.csv\", \"2024-04-10\", \"2025-04-10\")"
      ],
      "metadata": {
        "id": "h0q_7aTRrnwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# API call to retrieve extra features\n",
        "def retry_request(url, params=None, max_retries=3, delay=2):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            time.sleep(delay)\n",
        "    print(f\"All retries failed for URL: {url}\")\n",
        "    return None\n",
        "\n",
        "def get_historical_data(lat, lon, start_date, end_date):\n",
        "    url = (\n",
        "        f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
        "        f\"latitude={lat}&longitude={lon}\"\n",
        "        f\"&start_date={start_date}&end_date={end_date}\"\n",
        "        \"&daily=sunshine_duration,snowfall_sum,precipitation_sum,\"\n",
        "        \"surface_pressure_max,dew_point_2m_max,shortwave_radiation_sum,et0_fao_evapotranspiration\"\n",
        "        \"&timezone=auto\"\n",
        "    )\n",
        "    return retry_request(url)\n",
        "\n",
        "def process_data(input_path, output_path, subset_rows=None):\n",
        "    df = pd.read_csv(input_path)\n",
        "    df.columns = df.columns.str.lower()\n",
        "    subset = df if subset_rows is None else df.iloc[subset_rows]\n",
        "\n",
        "    with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"Coordinates\",\n",
        "            \"Max Precipitation (mm)\",\n",
        "            \"Total Sunshine (min)\",\n",
        "            \"Total Snowfall (mm)\",\n",
        "            \"Max Surface Pressure (hPa)\",\n",
        "            \"Max Dew Point (Â°C)\",\n",
        "            \"Total Shortwave Radiation (Wh/mÂ²)\",\n",
        "            \"Total Reference Evapotranspiration (mm)\"\n",
        "        ])\n",
        "\n",
        "        for i, row in subset.iterrows():\n",
        "            lat = float(row['latitude'])\n",
        "            lon = float(row['longitude'])\n",
        "\n",
        "            row_values = [\n",
        "                row.get(\"max precipitation (mm)\", \"N/A\"),\n",
        "                row.get(\"total sunshine (min)\", \"N/A\"),\n",
        "                row.get(\"total snowfall (mm)\", \"N/A\"),\n",
        "                row.get(\"max surface pressure (hpa)\", \"N/A\"),\n",
        "                row.get(\"max dew point (Â°c)\", \"N/A\"),\n",
        "                row.get(\"total shortwave radiation (wh/mÂ²)\", \"N/A\"),\n",
        "                row.get(\"total reference evapotranspiration (mm)\", \"N/A\"),\n",
        "            ]\n",
        "\n",
        "            if not any(pd.isna(val) for val in row_values):\n",
        "                print(f\"Skipped {i+1}/{len(subset)} ({lat},{lon}) â€” all values present.\")\n",
        "                writer.writerow([f\"{lat},{lon}\", *row_values])\n",
        "                continue\n",
        "\n",
        "            current_date = datetime(2025, 4, 15)\n",
        "            max_historical_date = (current_date - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
        "            date_ranges = [\n",
        "                (\"2024-04-10\", \"2025-04-09\"),\n",
        "                (\"2025-04-10\", max_historical_date)\n",
        "            ]\n",
        "\n",
        "            combined_data = {\n",
        "                'precipitation_sum': [],\n",
        "                'sunshine_duration': [],\n",
        "                'snowfall_sum': [],\n",
        "                'surface_pressure_max': [],\n",
        "                'dew_point_2m_max': [],\n",
        "                'shortwave_radiation_sum': [],\n",
        "                'et0_fao_evapotranspiration': []\n",
        "            }\n",
        "\n",
        "            data = None\n",
        "            for start, end in date_ranges:\n",
        "                if start > end:\n",
        "                    continue\n",
        "                data = get_historical_data(lat, lon, start, end)\n",
        "                if data:\n",
        "                    for key in combined_data:\n",
        "                        combined_data[key].extend(data.get('daily', {}).get(key, []))\n",
        "\n",
        "            et0_fao_evapotranspiration = [x for x in combined_data['et0_fao_evapotranspiration'] if x is not None]\n",
        "\n",
        "            max_precip = max(combined_data['precipitation_sum']) if combined_data['precipitation_sum'] else \"N/A\"\n",
        "            total_sun = sum(combined_data['sunshine_duration']) if combined_data['sunshine_duration'] else \"N/A\"\n",
        "            total_snow = sum(combined_data['snowfall_sum']) if combined_data['snowfall_sum'] else \"N/A\"\n",
        "            max_pressure = max(combined_data['surface_pressure_max']) if combined_data['surface_pressure_max'] else \"N/A\"\n",
        "            max_dew = max(combined_data['dew_point_2m_max']) if combined_data['dew_point_2m_max'] else \"N/A\"\n",
        "            total_irradiance = sum(combined_data['shortwave_radiation_sum']) if combined_data['shortwave_radiation_sum'] else \"N/A\"\n",
        "            total_et0 = sum(et0_fao_evapotranspiration) if et0_fao_evapotranspiration else \"N/A\"\n",
        "\n",
        "            writer.writerow([\n",
        "                f\"{lat},{lon}\",\n",
        "                max_precip,\n",
        "                total_sun,\n",
        "                total_snow,\n",
        "                max_pressure,\n",
        "                max_dew,\n",
        "                total_irradiance,\n",
        "                total_et0\n",
        "            ])\n",
        "\n",
        "            print(f\"Updated {i+1}/{len(subset)} ({lat},{lon}) â†’ Precip: {max_precip}, Sun: {total_sun}, Snow: {total_snow}, Pressure: {max_pressure}, Dew: {max_dew}, Irradiance: {total_irradiance}, ETâ‚€: {total_et0}\")\n",
        "            if not data:\n",
        "                print(f\"Failed for ({lat},{lon}). Waiting 30 seconds before moving to the next one.\")\n",
        "                time.sleep(30)\n",
        "            time.sleep(1.1)\n",
        "\n",
        "process_data(\"test.csv\", \"test2.csv\", subset_rows=range(0, 1000))\n"
      ],
      "metadata": {
        "id": "WL_AY9myrr3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from statistics import mean\n",
        "\n",
        "# API call to retrieve pollutants extra features\n",
        "def update_air_quality_data(input_csv, output_csv, columns, lat_col='latitude', lon_col='longitude', start_date='2024-04-10', end_date='2025-04-10'):\n",
        "    df_full = pd.read_csv(input_csv)\n",
        "    total = len(df_full)\n",
        "\n",
        "    for index, row in df_full.iterrows():\n",
        "        values = row[columns]\n",
        "        if values.notna().all():\n",
        "            continue\n",
        "\n",
        "        lat = row[lat_col]\n",
        "        lon = row[lon_col]\n",
        "\n",
        "        try:\n",
        "            url = 'https://air-quality-api.open-meteo.com/v1/air-quality'\n",
        "            params = {\n",
        "                'latitude': lat,\n",
        "                'longitude': lon,\n",
        "                'hourly': ','.join(columns),\n",
        "                'start_date': start_date,\n",
        "                'end_date': end_date,\n",
        "                'timezone': 'auto'\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            hourly = data.get('hourly', {})\n",
        "\n",
        "            def safe_avg(values):\n",
        "                return round(mean(v for v in values if v is not None), 2) if values else None\n",
        "\n",
        "            for col in columns:\n",
        "                avg_value = safe_avg(hourly.get(col, []))\n",
        "                df_full.loc[index, col] = avg_value\n",
        "\n",
        "            remaining = total - index - 1\n",
        "            print(f\"({index+1}/{total}) Updated (lat: {lat}, lon: {lon}) â€” {', '.join([f'{col}={df_full.loc[index, col]}' for col in columns])} â€” {remaining} remaining\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error at index {index} (lat: {lat}, lon: {lon}): {e}\")\n",
        "\n",
        "        if index % 100 == 0 and index > 0:\n",
        "            df_full.to_csv(output_csv, index=False)\n",
        "            print(f\"Progress saved at index {index}\")\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    df_full.to_csv(output_csv, index=False)\n",
        "    print(f\"All done. Data saved to '{output_csv}'\")\n",
        "\n",
        "update_air_quality_data('test.csv', 'test2.csv', ['PM2.5', 'NO2', 'SO2', 'Dust'])\n"
      ],
      "metadata": {
        "id": "uF4zRzgDrsEC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}